{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "991060a1",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de698982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.shape_base import split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92034cad",
   "metadata": {},
   "source": [
    "# 1/Decision Tree from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5159d0a8",
   "metadata": {},
   "source": [
    "# Entropy, Information Gain, Maximum Feature Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b470950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ent(data, clsf): #Calculating the entropy of each class and sum up\n",
    "    entropy = 0\n",
    "    #num of all data\n",
    "    num_all = data.shape[0] \n",
    "    ent=0;\n",
    "    for t in clsf:\n",
    "        clsfNum = data[data[Targetfeat] == t].shape[0] \n",
    "        clsfEnt = 0\n",
    "        if clsfNum != 0:\n",
    "            clsfProb = clsfNum / num_all\n",
    "            clsfEnt = - clsfProb * np.log2(clsfProb)\n",
    "        ent += clsfEnt\n",
    "    return ent\n",
    "\n",
    "\n",
    "def Information_Gain(featName, data, clsf): #Calculating I.G by subtracting total entropy from specific feature information    \n",
    "    feat_vals = np.unique(data[featName]) \n",
    "    num_all = data.shape[0] \n",
    "    feat_information = 0.0\n",
    "    \n",
    "    for val in feat_vals:\n",
    "        valData = data[data[featName] == val]\n",
    "        valNum = valData.shape[0]\n",
    "        valEntropy = Ent(valData, clsf) \n",
    "        valProbabilty = valNum / num_all\n",
    "        feat_information += valProbabilty * valEntropy \n",
    "        ent = Ent(data, clsf)\n",
    "    return ent - feat_information\n",
    "\n",
    "def Max_Info_Feature(data, clsf_list):\n",
    "    features = data.columns.drop(Targetfeat)\n",
    "    max_info_gain = -1\n",
    "    max_info_feature = None\n",
    "    for f in features:\n",
    "        featureIG = Information_Gain(f, data, clsf_list)\n",
    "        if max_info_gain < featureIG:\n",
    "            max_info_gain = featureIG\n",
    "            max_info_feature = f\n",
    "    return max_info_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e14a734",
   "metadata": {},
   "source": [
    "# Bulding Desicion Tree and Designing Recursive ID3 Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ba1fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Raw_Tree(featName, data, clsf, depth):\n",
    "    \n",
    "    feat_vals_nums = data[featName].value_counts(sort=False)\n",
    "    # Initilizing Tree\n",
    "    tree = {} \n",
    "    \n",
    "    for val, num in feat_vals_nums.iteritems():\n",
    "        valData = data[data[featName] == val]\n",
    "        flaged = False #flag for tracking value of feature is pure class or not\n",
    "        max_clsf_name = ''\n",
    "        max_clsf_num = 0\n",
    "        \n",
    "        for c in clsf:\n",
    "            clsf_count = valData[valData[Targetfeat] == c].shape[0]\n",
    "            \n",
    "            if clsf_count == num: \n",
    "                tree[val] = c \n",
    "                data = data[data[featName] != val] #removing rows with feature_value\n",
    "                flaged = True\n",
    "            if clsf_count > max_clsf_num:\n",
    "                max_clsf_num = clsf_count\n",
    "                max_clsf_name = c\n",
    "\n",
    "        if (flaged==0):\n",
    "            if depth != DEPTH-1:\n",
    "                tree[val] = \"?\"\n",
    "            else:\n",
    "                tree[val] = max_clsf_name         \n",
    "    return tree, data\n",
    "\n",
    "\n",
    "def Recurv_ID3(root, prev_feat_val, data, clsf, depth): #ID3 recursive algorithm\n",
    "    \n",
    "    if depth == DEPTH:\n",
    "        return\n",
    "\n",
    "    if data.shape[0] != 0: \n",
    "        max_inf_feat = Max_Info_Feature(data, clsf)\n",
    "        tree, data = Raw_Tree(max_inf_feat, data, clsf, depth)\n",
    "        next_root = None\n",
    "        \n",
    "        if prev_feat_val != None: \n",
    "            root[prev_feat_val] = dict()\n",
    "            root[prev_feat_val][max_inf_feat] = tree\n",
    "            next_root = root[prev_feat_val][max_inf_feat]\n",
    "        else: \n",
    "            root[max_inf_feat] = tree\n",
    "            next_root = root[max_inf_feat]\n",
    "        \n",
    "        for node, branch in list(next_root.items()): \n",
    "            if branch == \"?\":\n",
    "                feat_val_data = data[data[max_inf_feat] == node] \n",
    "                Recurv_ID3(next_root, node, feat_val_data, clsf, depth+1)\n",
    "\n",
    "\n",
    "def Generate_Tree(data):\n",
    "    tree = {}\n",
    "    clsf = data[Targetfeat].unique()\n",
    "    Recurv_ID3(tree, None, data, clsf, 0)\n",
    "    return tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80f362",
   "metadata": {},
   "source": [
    "# Spliting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a764c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Database_Spliting(data): # spliting data in 80-20 order for train and test \n",
    "    num_all = data.shape[0] \n",
    "    split_ind = int(80 / 100 * num_all)\n",
    "    test_data = data.iloc[split_ind:].reset_index(drop=True)\n",
    "    train_data = data.iloc[0:split_ind].reset_index(drop=True)\n",
    "    return test_data, train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109a8f4",
   "metadata": {},
   "source": [
    "# Prediction, Confusion Matrix & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1977db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(features, tree): \n",
    "        for key in list(features.keys()):\n",
    "            if key in list(tree.keys()):\n",
    "                result = tree[key][features[key]]\n",
    "                if isinstance(result, dict):\n",
    "                    return Predict(features, result)\n",
    "                else:\n",
    "                    return result\n",
    "\n",
    "def Confusion_Matrix(prediction, targets): \n",
    "    conf = np.zeros((num_types, num_types), dtype=np.int32)\n",
    "    Prediction = np.array(prediction)\n",
    "    Target = np.array(targets)\n",
    "    \n",
    "    for i in range(len(Prediction)):\n",
    "        if Prediction[i] == 1: \n",
    "            if Target[i] == 1:\n",
    "                conf[0][0] += 1\n",
    "            elif Target[i] == 0:\n",
    "                conf[0][1] += 1\n",
    "        elif Prediction[i] == 0:\n",
    "            if Target[i] == 1:\n",
    "                conf[1][0] += 1\n",
    "            elif Target[i] == 0:\n",
    "                conf[1][1] += 1\n",
    "    print(conf)\n",
    "\n",
    "def Test_ConfusionMatrix(test, tree):\n",
    "    features_test = test.iloc[:,:-1] #Droping Target Column \n",
    "    #print (features_test)\n",
    "    features_test = features_test.to_dict(orient = \"records\")\n",
    "    #print (features_test)\n",
    "    prediction = pd.DataFrame(columns=[\"Predict\"]) \n",
    "    for i in range(len(test)):\n",
    "        prediction.loc[i,\"Predict\"] = Predict(features_test[i], tree)\n",
    "    print ('Accuracy: ',(np.sum(prediction[\"Predict\"] == test[Targetfeat])/len(test))*100)\n",
    "    Confusion_Matrix(prediction, test[Targetfeat])\n",
    "    return prediction[\"Predict\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbf200a",
   "metadata": {},
   "source": [
    "# Loading Data and Preparing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "262e978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'E:\\university\\Term_7\\AI\\HW\\HW2\\HW2\\prison_dataset.csv') #load dataset\n",
    "\n",
    "featNum = data.shape[1] #number of columns(features)\n",
    "featNames = list(data.columns) #names of columns(features)\n",
    "\n",
    "data = data.sample(frac = 1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55563473",
   "metadata": {},
   "source": [
    "# Defining Global Variables (Depth and Target Feature) & Terminal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af785438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fiscal Year Released': {2010: 1, 2013: {'Age At Release': {'>45': {'Convicting Offense Type': {'Other': 1, 'Violent': 1, 'Drug': 1}}, '<45': {'Main Supervising District': {'3JD': 1, '5JD': 1}}}}, 2015: {'Release Type': {'Parole': {'Age At Release': {'>45': 0, '<45': 0}}, 'Discharged End of Sentence': {'Main Supervising District': {'3JD': 0, '5JD': 0}}}}}}\n",
      "Accuracy:  72.86871961102108\n",
      "[[1088  198]\n",
      " [ 639 1160]]\n"
     ]
    }
   ],
   "source": [
    "DEPTH = 3; \n",
    "Targetfeat = 'Recidivism - Return to Prison numeric'\n",
    "num_types = data[Targetfeat].nunique()\n",
    "\n",
    "Test_Data, Training_Data = Database_Spliting(data)\n",
    "Decision_Tree = Generate_Tree(Training_Data.copy())\n",
    "print (Decision_Tree)\n",
    "ACC = Test_ConfusionMatrix(Test_Data, Decision_Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fce31d",
   "metadata": {},
   "source": [
    "# 2/ Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c82192",
   "metadata": {},
   "source": [
    "# Spliting Train Data to 4 Parts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c35b6c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomF_Prep(data): # spliting data to 4 parts \n",
    "    num_all = data.shape[0] \n",
    "    split_ind = int(25 / 100 * num_all)\n",
    "    train_data1 = data.iloc[0:split_ind].reset_index(drop=True)\n",
    "    train_data2 = data.iloc[split_ind:2*split_ind].reset_index(drop=True)\n",
    "    train_data3 = data.iloc[2*split_ind:3*split_ind].reset_index(drop=True)\n",
    "    train_data4 = data.iloc[3*split_ind:4*split_ind].reset_index(drop=True)\n",
    "    return train_data1, train_data2, train_data3, train_data4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e39b8",
   "metadata": {},
   "source": [
    "# Designing Random Forest & Calculating Accuracy and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4008b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest(P1,P2,P3,P4):\n",
    "    majority = pd.DataFrame(columns=[\"Predict\"]) \n",
    "    P5 = P1+ P2+ P3+ P4\n",
    "    for i in range(len(P1)):\n",
    "        if (P5[i]>2):\n",
    "            majority.loc[i,\"Predict\"]=1\n",
    "        else:\n",
    "            majority.loc[i,\"Predict\"]=0\n",
    "    return majority\n",
    "\n",
    "def RandomForest_Test_ConfusionMatrix(prediction,test):\n",
    "    print ('Accuracy: ',(np.sum(prediction[\"Predict\"] == test[Targetfeat])/len(test))*100)\n",
    "    Confusion_Matrix(prediction, test[Targetfeat])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb1576",
   "metadata": {},
   "source": [
    "# Applying the \"From Scratch\" Random Forest Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa72d184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  72.99837925445705\n",
      "[[1337  435]\n",
      " [ 398  915]]\n",
      "Accuracy:  72.15559157212319\n",
      "[[1297  421]\n",
      " [ 438  929]]\n",
      "Accuracy:  70.98865478119936\n",
      "[[1225  385]\n",
      " [ 510  965]]\n",
      "Accuracy:  73.22528363047002\n",
      "[[1236  327]\n",
      " [ 499 1023]]\n",
      "Accuracy:  73.54943273905997\n",
      "[[1260  341]\n",
      " [ 475 1009]]\n"
     ]
    }
   ],
   "source": [
    "data_F = pd.read_csv(r'E:\\university\\Term_7\\AI\\HW\\HW2\\HW2\\prison_dataset.csv') #load dataset\n",
    "data_F = data_F.sample(frac = 1).reset_index(drop = True)\n",
    "Test_Data_F, Training_Data_F = Database_Spliting(data_F)\n",
    "train_data1, train_data2, train_data3, train_data4 = RandomF_Prep(Training_Data_F)\n",
    "\n",
    "Tree1 = Generate_Tree(train_data1.copy())\n",
    "P1 = Test_ConfusionMatrix(Test_Data_F, Tree1)\n",
    "Tree2 = Generate_Tree(train_data2.copy())\n",
    "P2 = Test_ConfusionMatrix(Test_Data_F, Tree2)\n",
    "Tree3 = Generate_Tree(train_data3.copy())\n",
    "P3 = Test_ConfusionMatrix(Test_Data_F, Tree3)\n",
    "Tree4 = Generate_Tree(train_data4.copy())\n",
    "P4 = Test_ConfusionMatrix(Test_Data_F, Tree4)\n",
    "\n",
    "Random_F = Random_Forest (P1,P2,P3,P4)\n",
    "RandomForest_Test_ConfusionMatrix(Random_F,Test_Data_F)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36241559",
   "metadata": {},
   "source": [
    "# Applying the \"Library Base\" Random Forest Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3ca4451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  70.34035656401944\n",
      "[[1101  253]\n",
      " [ 662 1069]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "dataset = pd.read_csv(r'E:\\university\\Term_7\\AI\\HW\\HW2\\HW2\\prison_dataset.csv')\n",
    "Test_Data, Training_Data = Database_Spliting(data)\n",
    "\n",
    "train_features = Training_Data.iloc[:,:-1]\n",
    "test_features = Test_Data.iloc[:,:-1]\n",
    "train_targets = Training_Data.iloc[:,-1]\n",
    "test_targets = Test_Data.iloc[:,-1]\n",
    "\n",
    "for f in train_features.keys():\n",
    "    train_features[f] = le.fit_transform(train_features[f])\n",
    "    test_features[f] = le.fit_transform(test_features[f])\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion = 'entropy',max_depth = 3).fit(train_features,train_targets)\n",
    "prediction = tree.predict(test_features)\n",
    "\n",
    "print(\"Accuracy: \",tree.score(test_features,test_targets)*100)\n",
    "Confusion_Matrix(prediction, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af60fca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593592c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f335a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
